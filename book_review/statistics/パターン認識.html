<!DOCTYPE html>
<html lang="ja">
<head>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6792495598564208"
     crossorigin="anonymous"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JPYT21F66Y"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JPYT21F66Y');
</script>
  <meta charset="UTF-8">
  <link rel="stylesheet" type="text/css" href="../../stylesheets/base.css">
  <link rel="stylesheet" type="text/css" href="../../stylesheets/book_review.css">
  <title>miyake.github.io | 本の紹介 | パターン認識</title>
</head>

<body>

  <h1>本の紹介 : パターン認識</h1>

  <hr />
  <h3>
  平井有三「はじめてのパターン認識」 (2012)
  </h3>

  <p class="toc">
  1. はじめに <br />
  2. 識別規則と学習法の概要 <br />
  3. ベイズの識別規則 <br />
  4. 確率モデルと識別関数 <br />
  5. k最近傍法 (kNN法) <br />
  6. 線形識別関数 <br />
  7. パーセプトロン型学習規則 <br />
  8. サポートベクトルマシン <br />
  9. 部分空間法 <br />
  10. クラスタリング <br />
  11. 識別器の組み合わせによる性能強化 <br />
  A. ベクトルと行列による微分
  </p>

  <hr />
  <h3>
    石井健一郎・前田英作・上田修功・村瀬洋
    「わかりやすいパターン認識」 (1998)
  </h3>

  <p class="toc">
  1. パターン認識とは <br />
  2. 学習と識別関数 <br />
  3. 誤差評価に基づく学習 <br />
  4. 識別部の設計 <br />
  5. 特徴の評価とベイズ誤り確率 <br />
  6. 特徴空間の変換 <br />
  7. 部分空間法 <br />
  8. 学習アルゴリズムの一般化 <br />
  9. 学習アルゴリズムとベイズ決定則
  </p>

  <hr />
  <h3>
    石井健一郎・上田修功
    「続・わかりやすいパターン認識 - 教師なし学習入門」 (2014)
  </h3>

  <p class="toc">
  1. ベイズ統計学 <br />
  2. 事前確率と事後確率 <br />
  3. ベイズ決定則 <br />
  4. パラメータ推定 <br />
  5. 教師付き学習と教師なし学習 <br />
  6. EMアルゴリズム <br />
  7. マルコフモデル <br />
  8. 隠れマルコフモデル <br />
  9. 混合分布のパラメータ推定 <br />
  10. クラスタリング <br />
  11. ノンパラメトリックベイズモデル <br />
  12. ディリクレ過程混合モデルによるクラスタリング <br />
  13. 共クラスタリング
  </p>

  <hr />
  <h3>
    金森敬文・竹之内高志・村田昇
    「パターン認識 (Rで学ぶデータサイエンス 5)」 (2009)
  </h3>

  <p class="toc">
  1. 判別能力の評価 <br />
  2. k-平均法 <br />
  3. 階層的クラスタリング <br />
  4. 混合正規分布モデル <br />
  5. 判別分析 <br />
  6. ロジスティック回帰 <br />
  7. 密度推定 <br />
  8. k-近傍法 <br />
  9. 学習ベクトル量子化 <br />
  10. 決定木 <br />
  11. サポートベクターマシン <br />
  12. 正則化とパス追跡アルゴリズム <br />
  13. ミニマックス確率マシン <br />
  14. 集団学習 <br />
  15. 2値判別から多値判別へ
  </p>

  <hr />
  <h3>
  <a href="https://www.amazon.co.jp/dp/0387310738/ref=nosim?tag=msscee0a-22">
    Bishop, Pattern Recognition and Machine Learning (2006)
  </a>
  </h3>

  <p class="toc">
  1. Introduction <br />
  2. Probability Distributions <br />
  3. Linear Models for Regression <br />
  4. Linear Models for Classification <br />
  5. Neural Networks <br />
  6. Kernel Methods <br />
  7. Sparse Kernel Machines <br />
  8. Graphical Models <br />
  9. Mixture Models and EM <br />
  10. Approximate Inference <br />
  11. Sampling Methods <br />
  12. Continuous Latent Variables <br />
  13. Sequential Data <br />
  14. Combining Models
  </p>

  <h3>
  <a href="https://www.amazon.co.jp/dp/4621061224/ref=nosim?tag=msscee0a-22">
    Bishop 「パターン認識と機械学習 上」 (2012)
  </a>
  <a href="https://www.amazon.co.jp/dp/4621061240/ref=nosim?tag=msscee0a-22">
    「パターン認識と機械学習 下」 (2012)
  </a>
  </h3>

  <hr />
  <h3>
  <a href="https://www.amazon.co.jp/dp/0387848576/ref=nosim?tag=msscee0a-22">
    The Elements of Statistical Learning, 2nd (2008)
  </a>
  （
  <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">
    本のページ
  </a>
  ）
  <br />
  <a href="https://www.amazon.co.jp/dp/432012362X/ref=nosim?tag=msscee0a-22">
    「統計的学習の基礎―データマイニング・推論・予測―」 (2014)
</a>
  </h3>

  <p class="toc">
  1. Introduction <br />
  2. Overview of Supervised Learning <br />
  3. Linear Methods of Regression <br />
  4. Linear Methods for Classification <br />
  5. Basis Expansion and Regularization <br />
  6. Kernel Smoothing Methods <br />
  7. Model Assessment and Selection <br />
  8. Model Inference and Averaging <br />
  9. Additive Models, Trees, and Related Methods <br />
  10. Boosting and Additive Trees <br />
  11. Neural Networks <br />
  12. Support Vector Machines and Flexible Discriminants <br />
  13. Prototype Methods and Nearest-Neighbors <br />
  14. Unsupervised Learning <br />
  15. Random Forests <br />
  16. Ensemble Learning <br />
  17. Undirected Graphical Models <br />
  18. High-Dimensional Problems : p>>N
  </p>

  <hr />
  <h3>
  <a href="https://www.amazon.co.jp/dp/0262018020/ref=nosim?tag=msscee0a-22">
    Murphy, Machine Learning: A Probabilistic Perspective (2012)
  </a>
  </h3>

  <p class="toc">
  1. Introduction <br />
  2. Probability <br />
  3. Generative models for discrete data <br />
  4. Gaussian models <br />
  5. Bayesian statistics <br />
  6. Frequentist statistics <br />
  7. Linear regression <br />
  8. Logistic regression <br />
  9. Generalized linear models and the exponential family <br />
  10. Directed graphical models (Bayes nets) <br />
  11. Mixture models and the EM algorithm <br />
  12. Latent linear models <br />
  13. Sparse linear models <br />
  14. Kernels <br />
  15. Gaussian processes <br />
  16. Adaptive basis function models <br />
  17. Markov and hidden Markov models <br />
  18. State space models <br />
  19. Undirected graphical models (Markov random fields) <br />
  20. Exact inference for graphical models <br />
  21. Variational inference <br />
  22. More variational inference <br />
  23. Monte Carlo inference <br />
  24. Markov chain Monte Carlo (MCMC) inference <br />
  25. Clustering <br />
  26. Graphical model structure learning <br />
  27. Latent variable models for discrete data <br />
  28. Deep learning
  </p>

  <hr />
  <p>
  <a href="../../index.html">ホーム</a>
  </p>

</body>
</html>

